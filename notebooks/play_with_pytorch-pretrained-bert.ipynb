{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playing with pytorch-pretrained-BERT for Japanese language\n",
    "\n",
    "In this notebook, we show how to play with [pytorch-pretrained-BERT](https://github.com/huggingface/pytorch-pretrained-BERT) for Japanese language. We use Kyoto pretrained BERT model. Kyoto pretrained BERT model does not use BPE (subwords) in tokenization. According to the guideline, you need to comment the following line in the file `pytorch-pretrained-BERT/tokenization.py`\n",
    "\n",
    "    # text = self._tokenize_chinese_chars(text)\n",
    "\n",
    "I changed the `tokenization.py` so that it can be used for Japanese and other languages as well. The modification is the use of the argument `kyoto_bert=[True|False]`. You just need to installed from the forked repo [https://github.com/minhpqn/pytorch-pretrained-BERT](https://github.com/minhpqn/pytorch-pretrained-BERT).\n",
    "\n",
    "You also need to do tokenization with [Juman++](https://github.com/ku-nlp/jumanpp) before using BERT model.\n",
    "\n",
    "You need to install pytorch-pretrained-BERT in order to run the notebook.\n",
    "\n",
    "## Load pretrained BERT model for Japanese\n",
    "\n",
    "We now load [pretrained BERT model](http://nlp.ist.i.kyoto-u.ac.jp/index.php?BERT%E6%97%A5%E6%9C%AC%E8%AA%9EPretrained%E3%83%A2%E3%83%87%E3%83%AB) published by Kyoto University.\n",
    "\n",
    "Note that, we have to set `do_lower_case=False`, we may lost voiced consonant marks (nigori) of Japanese."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_pretrained_bert.tokenization:loading vocabulary file /Users/minhpqn/workspace/Japanese_L-12_H-768_A-12_E-30_BPE/vocab.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "['[CLS]', '数学', 'の', '最も', '普通の', '定義', 'と', 'して', 'は', '、', '「', '数', 'および', '図形', 'に', 'ついて', 'の', '学問', '」', 'と', 'いう', 'もの', 'が', 'ある', '。']\n",
      "['[CLS]', '数学', 'の', '最も', '普通の', '定義', 'と', 'して', '[MASK]', '、', '「', '数', 'および', '図形', 'に', 'ついて', 'の', '学問', '」', 'と', 'いう', 'もの', 'が', 'ある', '。']\n",
      "[2, 2938, 5, 476, 7078, 1315, 12, 19, 4, 6, 24, 145, 186, 17201, 8, 130, 5, 5476, 25, 12, 56, 60, 11, 38, 7]\n",
      "torch.Size([1, 25])\n",
      "tensor([[    2,  2938,     5,   476,  7078,  1315,    12,    19,     4,     6,\n",
      "            24,   145,   186, 17201,     8,   130,     5,  5476,    25,    12,\n",
      "            56,    60,    11,    38,     7]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from pytorch_pretrained_bert import BasicTokenizer, BertTokenizer, BertModel, BertForMaskedLM\n",
    "from pyknp import Juman\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "jumanpp = Juman()\n",
    "\n",
    "def tokenize(text):\n",
    "    result = jumanpp.analysis(text)\n",
    "    tokens = []\n",
    "    for mrph in result.mrph_list():\n",
    "        tokens.append(mrph.midasi)\n",
    "    return ' '.join(tokens)\n",
    "        \n",
    "path_to_pretrained_model = '/Users/minhpqn/workspace/Japanese_L-12_H-768_A-12_E-30_BPE'\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained(path_to_pretrained_model, kyoto_bert=True, do_lower_case=False)\n",
    "\n",
    "# Tokenized input\n",
    "text = \"数学の最も普通の定義としては、「数および図形についての学問」というものがある。\"\n",
    "text = tokenize(text)\n",
    "text = '[CLS] ' + text\n",
    "tokenized_text = tokenizer.tokenize(text)\n",
    "print(tokenized_text)\n",
    "\n",
    "# Mask a token that we will try to predict back with `BertForMaskedLM`\n",
    "masked_index = 8\n",
    "tokenized_text[masked_index] = '[MASK]'\n",
    "\n",
    "print(tokenized_text)\n",
    "\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "tokens_tensor = tokens_tensor.to(device)\n",
    "\n",
    "print(indexed_tokens)\n",
    "print(tokens_tensor.size())\n",
    "print(tokens_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how to use ```BertModel``` to get hidden states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_pretrained_bert.modeling:loading archive file /Users/minhpqn/workspace/Japanese_L-12_H-768_A-12_E-30_BPE\n",
      "INFO:pytorch_pretrained_bert.modeling:Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 32006\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(32006, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): BertLayerNorm()\n",
       "    (dropout): Dropout(p=0.1)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load pre-trained model (weights)\n",
    "model = BertModel.from_pretrained(path_to_pretrained_model, cache_dir=None)\n",
    "model.eval()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get hidden states of the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([25, 768])\n",
      "torch.Size([1, 768])\n",
      "torch.Size([768])\n",
      "tensor([[-2.6145e-01,  1.6846e-01, -9.9489e-01,  1.6224e-01,  6.5779e-02,\n",
      "         -1.6521e-01, -1.0453e-01, -6.4537e-02, -8.9345e-01, -4.5911e-01,\n",
      "         -2.0780e-01,  9.0893e-01,  7.4370e-02,  9.9050e-01,  3.2349e-01,\n",
      "         -6.2441e-02,  7.7569e-02, -2.4102e-01, -9.9853e-01, -1.8675e-02,\n",
      "          9.1260e-01, -2.4392e-01, -8.3436e-01,  1.3213e-01, -7.2599e-01,\n",
      "          1.0340e-01, -6.5891e-02,  3.3977e-01,  1.7579e-02,  9.6078e-01,\n",
      "         -3.5215e-01,  2.1684e-02, -1.7530e-01, -9.3593e-01,  2.3440e-01,\n",
      "         -2.9097e-02, -9.0766e-01, -4.3840e-02, -1.9927e-01, -4.9560e-01,\n",
      "         -1.8500e-02,  1.0180e-01,  1.4831e-01,  3.8736e-03,  6.5150e-01,\n",
      "         -1.8352e-01,  5.3526e-02, -1.1046e-01,  8.6650e-01, -9.1555e-02,\n",
      "          6.6434e-01,  1.7088e-01,  1.1503e-02, -9.9135e-01,  7.7298e-01,\n",
      "          1.3503e-01,  8.0489e-02, -2.3020e-02, -8.9470e-02,  9.9903e-01,\n",
      "          5.9107e-02,  4.3549e-02, -1.0930e-01,  4.0429e-02,  1.9625e-01,\n",
      "          4.5314e-01,  2.0491e-01,  8.7510e-01,  8.9825e-01,  4.3244e-01,\n",
      "          2.1073e-01,  8.6815e-01,  9.6033e-02, -2.6798e-01, -7.8831e-01,\n",
      "         -6.1267e-03,  1.3887e-01,  7.8455e-02,  9.9596e-01, -8.4396e-02,\n",
      "          1.3370e-01, -8.4773e-01,  1.3162e-01, -7.1100e-02,  3.6984e-01,\n",
      "         -2.6455e-01, -2.1696e-01, -6.8964e-01,  4.9534e-01, -7.7175e-02,\n",
      "          8.0908e-01,  9.1396e-02, -3.1913e-01, -7.0979e-01, -6.9809e-01,\n",
      "         -6.2011e-02, -1.7310e-01,  1.5945e-01, -8.9019e-01, -9.8146e-02,\n",
      "          2.1978e-01,  1.8399e-01, -1.7036e-01,  8.3779e-01,  1.3959e-01,\n",
      "         -5.3333e-01, -8.5557e-01,  7.5377e-01,  3.9619e-02, -1.8272e-01,\n",
      "         -8.4323e-02,  2.6207e-01,  2.1797e-01, -8.0968e-01, -3.0646e-02,\n",
      "          8.5175e-01, -6.0904e-01,  7.1066e-01, -2.0793e-02,  2.1988e-01,\n",
      "         -2.9978e-01,  9.3346e-01,  2.8508e-01, -2.1699e-01, -9.1123e-01,\n",
      "         -2.4929e-02,  1.8547e-01,  2.1127e-01,  9.2198e-01,  1.6925e-01,\n",
      "         -6.2828e-01, -8.8822e-02,  1.7762e-01, -9.5796e-02, -4.4860e-02,\n",
      "          2.3219e-01, -1.6569e-04, -1.7069e-02, -9.4529e-01, -2.9153e-01,\n",
      "         -3.3943e-01, -1.0407e-01, -9.9257e-01,  9.5864e-01, -1.9333e-01,\n",
      "          9.9310e-01, -6.1121e-02, -9.7604e-01, -1.6904e-02,  2.2339e-01,\n",
      "         -1.5256e-01, -3.6495e-01, -1.4633e-01, -2.4227e-01,  4.0786e-01,\n",
      "         -4.2674e-01,  8.4344e-01, -2.8704e-01,  2.1333e-01,  2.3013e-01,\n",
      "          8.0019e-01,  9.5031e-01,  2.3678e-01, -8.0466e-01,  7.5049e-03,\n",
      "         -2.0913e-01, -3.5389e-01,  2.2818e-01, -8.3323e-02,  1.4999e-01,\n",
      "          1.0940e-01,  2.2910e-01,  3.0734e-01, -6.8815e-02, -4.2345e-01,\n",
      "         -1.1871e-01,  2.9329e-01,  3.7396e-02, -2.0407e-01,  9.2767e-01,\n",
      "          1.1076e-01, -7.3679e-01,  9.2721e-01,  4.2215e-02,  1.3801e-01,\n",
      "         -1.2836e-01, -7.4883e-02, -9.8324e-01, -8.7369e-01,  1.3251e-01,\n",
      "         -3.3883e-01, -9.8752e-01,  9.4152e-01,  9.6659e-01, -9.9896e-01,\n",
      "         -1.5452e-01,  1.4232e-01,  6.4537e-01,  7.4703e-01, -2.4375e-01,\n",
      "         -6.4408e-02, -1.3701e-01,  1.2047e-01, -1.6875e-01, -2.4435e-02,\n",
      "         -3.5329e-01, -2.2172e-01,  7.6627e-02, -8.4623e-01, -9.7924e-01,\n",
      "         -1.0348e-01, -3.0702e-01,  1.0382e-01, -1.7742e-01, -3.1251e-01,\n",
      "          9.6609e-01, -3.4123e-02,  2.3870e-01, -9.5347e-02,  6.1955e-03,\n",
      "          2.1504e-01,  6.0233e-02,  3.4737e-01,  2.1152e-01,  3.0414e-01,\n",
      "          6.4587e-02,  7.6950e-01, -2.4372e-01, -2.4992e-02,  3.7567e-01,\n",
      "         -2.3437e-01, -7.5354e-02, -8.5159e-01,  9.4649e-01,  1.5361e-01,\n",
      "          2.8043e-01,  1.1485e-02,  9.5317e-01, -8.6794e-02,  8.1602e-02,\n",
      "          2.1219e-01, -3.8213e-02,  1.5081e-01, -3.0660e-01, -6.0020e-01,\n",
      "          9.0884e-01, -1.2324e-01,  2.4175e-02,  7.4258e-02,  1.0903e-01,\n",
      "         -8.3693e-01, -9.9614e-01, -3.2019e-01,  4.5449e-02,  1.7912e-01,\n",
      "         -2.3737e-01, -4.0203e-03,  1.1028e-01, -3.1457e-01,  1.6940e-01,\n",
      "          9.1988e-01, -1.1427e-01, -5.0720e-01,  4.8884e-02,  7.6197e-01,\n",
      "          2.2074e-01,  1.8167e-01,  2.0505e-01, -2.8259e-01, -3.9430e-01,\n",
      "         -9.5046e-01,  8.4651e-01, -6.6237e-01,  8.6031e-01,  1.6049e-01,\n",
      "         -9.8883e-01,  2.1043e-01,  2.0585e-01,  2.5633e-01,  4.5704e-01,\n",
      "          1.3665e-01, -1.8147e-01,  7.0482e-02, -4.1987e-02, -4.6224e-01,\n",
      "          6.7249e-01,  3.0990e-01, -6.0048e-01, -8.2916e-01, -9.7363e-01,\n",
      "         -1.8125e-01, -2.9285e-01, -3.0231e-01,  1.4732e-01,  1.1079e-01,\n",
      "         -3.0567e-01, -5.6031e-01, -9.0853e-01,  9.8956e-02, -7.2587e-02,\n",
      "          8.9471e-01, -8.6241e-02,  1.3345e-01, -2.7476e-01, -2.8769e-01,\n",
      "         -1.5718e-01,  6.8158e-03, -1.4146e-01,  3.4198e-02, -7.4098e-02,\n",
      "         -1.1163e-01, -9.9767e-01, -9.0670e-01,  1.9523e-01,  1.5400e-01,\n",
      "          1.7591e-01, -3.8873e-01,  4.2571e-01, -7.2897e-01,  6.2729e-01,\n",
      "         -1.1341e-02,  9.6431e-01,  9.2958e-01, -8.0327e-01, -2.8932e-01,\n",
      "         -5.2416e-02, -2.4865e-01, -9.0422e-01,  3.0484e-01,  1.9139e-01,\n",
      "         -1.0173e-01,  2.1539e-01, -1.2521e-01,  1.9066e-01, -2.9310e-01,\n",
      "          1.1470e-01,  5.3708e-03, -2.9253e-01, -3.7108e-02, -2.7144e-01,\n",
      "         -8.1770e-02, -4.5140e-01,  5.6617e-02,  1.6240e-01, -4.7249e-01,\n",
      "          2.7188e-01,  7.6211e-01,  2.3322e-01,  1.3623e-01, -1.8288e-01,\n",
      "          1.9598e-01, -5.3305e-01,  4.3205e-01, -1.4172e-01,  1.0312e-01,\n",
      "         -5.7697e-01,  2.0671e-01, -1.2929e-01,  7.7613e-02,  2.2520e-02,\n",
      "         -2.6878e-01, -5.9263e-01, -1.3484e-01, -1.5761e-01,  2.2760e-01,\n",
      "          2.8092e-01, -6.7082e-01, -6.5968e-01,  5.7692e-01,  9.9652e-01,\n",
      "         -1.2534e-02, -6.8776e-01, -8.2154e-01, -4.3006e-01,  5.6796e-02,\n",
      "          1.6077e-01,  1.8702e-01, -5.6521e-03,  8.3473e-01, -1.1179e-01,\n",
      "          7.9895e-03,  4.9411e-01, -9.5912e-01, -4.2621e-01,  8.8303e-01,\n",
      "         -2.3663e-01, -6.2011e-02, -3.3967e-01,  2.9331e-01,  8.2005e-02,\n",
      "         -8.7378e-02, -5.7771e-03, -1.8402e-01,  6.0407e-01, -9.8646e-01,\n",
      "         -1.7909e-01,  1.8858e-01,  7.9469e-02,  7.1010e-01,  2.8607e-01,\n",
      "          9.6016e-02, -9.1465e-01,  2.8890e-01, -6.4665e-02,  1.1224e-01,\n",
      "         -1.7581e-01,  9.0801e-02,  2.4720e-01, -3.0406e-01,  5.4812e-02,\n",
      "         -1.7895e-03, -2.6496e-01,  7.6012e-01,  9.8355e-01, -6.9876e-01,\n",
      "          9.2546e-01,  1.6055e-01, -7.8536e-02,  4.8264e-01,  2.1226e-01,\n",
      "         -1.1600e-02,  6.1331e-01,  7.7932e-01, -8.8285e-01, -9.9555e-01,\n",
      "         -8.1168e-01, -7.1079e-01, -8.5456e-01,  5.0023e-02,  9.8479e-01,\n",
      "          4.0617e-01, -9.9409e-01, -9.7539e-01,  9.5427e-02,  9.8005e-01,\n",
      "          1.7620e-01,  5.7382e-02,  6.0426e-01, -1.4875e-01,  3.6056e-02,\n",
      "          5.2882e-01, -5.5182e-01,  3.4204e-03,  8.4282e-01,  7.8060e-01,\n",
      "         -3.6932e-01, -6.5512e-01, -1.8522e-01, -1.4119e-01, -6.5527e-01,\n",
      "         -1.4852e-01, -2.0875e-01,  9.4535e-01, -5.3511e-01,  7.5858e-01,\n",
      "         -9.7363e-01, -2.4556e-01, -9.3578e-01,  4.2133e-01,  9.7355e-01,\n",
      "         -3.0346e-01, -4.0605e-03, -1.9593e-01, -9.2253e-02, -8.9969e-02,\n",
      "          1.3236e-01, -7.7240e-01, -1.7246e-01,  9.0939e-01, -8.9390e-02,\n",
      "          4.9823e-01, -7.2316e-01, -2.5125e-01, -4.2118e-01, -2.5844e-01,\n",
      "          9.6607e-01,  9.5322e-02, -5.0370e-02, -1.9090e-01, -8.0482e-01,\n",
      "          5.2281e-01,  9.4025e-01,  2.4671e-01,  8.6075e-01, -6.2270e-02,\n",
      "          9.5119e-01,  3.5870e-01,  2.5900e-01, -6.5225e-01, -2.0898e-01,\n",
      "         -6.3530e-01,  2.3449e-02, -9.6119e-01, -6.3948e-02,  1.0164e-01,\n",
      "         -8.7138e-01,  4.4568e-01,  6.5641e-03, -9.1986e-01, -3.6309e-01,\n",
      "         -8.7950e-01,  4.3491e-02,  4.4512e-01,  3.0717e-01, -6.8675e-02,\n",
      "          9.3418e-01,  9.2707e-01,  1.0791e-01,  8.7382e-02, -6.1086e-01,\n",
      "          2.6982e-01,  1.5781e-01, -1.4716e-01, -9.0185e-01, -7.0054e-02,\n",
      "          2.6893e-01, -9.4401e-01,  1.7630e-01,  6.9684e-01, -4.6369e-02,\n",
      "          5.4824e-02,  8.9971e-01,  4.1035e-01, -1.2966e-01, -1.0924e-01,\n",
      "          3.7463e-02,  2.1325e-01, -1.1217e-01, -1.0417e-01,  1.0080e-01,\n",
      "         -1.6044e-01,  2.4149e-02,  7.5029e-02, -1.7031e-01, -2.2767e-02,\n",
      "         -6.9510e-01,  8.0405e-02, -2.6165e-01, -7.0341e-01,  2.6101e-01,\n",
      "          2.2182e-01,  3.1050e-02, -6.4905e-02, -2.2408e-01,  3.9124e-02,\n",
      "         -2.5180e-02,  8.7534e-02, -2.0557e-01, -1.3128e-01,  3.7845e-01,\n",
      "         -2.5539e-01,  1.5275e-01,  1.9954e-01, -7.8725e-01,  4.2840e-01,\n",
      "         -1.9238e-01, -1.4748e-01,  2.3327e-02, -9.1912e-01,  9.7800e-02,\n",
      "          2.6184e-03,  1.0303e-01, -7.2969e-01, -7.1471e-01, -2.1748e-01,\n",
      "          4.0711e-01, -2.1891e-01,  8.7113e-01, -9.8902e-01, -8.8515e-02,\n",
      "          1.2407e-01, -7.4499e-02,  2.1259e-02, -2.9424e-01,  9.9967e-01,\n",
      "          1.9793e-01, -2.8650e-02,  8.7740e-01, -9.9899e-01,  5.0860e-02,\n",
      "         -2.5654e-01, -2.1519e-01, -2.2339e-02, -1.6103e-02,  7.5320e-01,\n",
      "         -4.9827e-02,  3.0953e-01, -2.1810e-01,  1.1953e-01,  1.8151e-02,\n",
      "         -9.5032e-01,  9.9983e-01,  1.0373e-01, -5.9331e-02, -7.2177e-01,\n",
      "         -7.8899e-02,  9.9443e-01, -2.3578e-01,  9.7770e-01, -1.1176e-02,\n",
      "         -2.9572e-01, -1.3240e-01, -9.6619e-01, -7.7648e-02,  9.2632e-01,\n",
      "         -7.6472e-01, -4.2871e-01, -8.6946e-01, -9.2801e-03,  5.2200e-02,\n",
      "          1.8031e-01, -7.8514e-02, -2.1292e-01, -3.2952e-01,  8.9024e-01,\n",
      "          4.0009e-01, -1.3478e-01, -1.4229e-01, -3.3259e-01, -4.6028e-01,\n",
      "          5.8714e-01, -7.3479e-01,  1.6257e-01, -9.8495e-01, -4.9572e-02,\n",
      "         -3.7582e-01, -1.9674e-01, -1.4172e-01, -8.0720e-01,  8.5748e-01,\n",
      "          1.0349e-03,  3.5892e-01,  3.7459e-01,  2.1049e-01,  8.9883e-01,\n",
      "          8.1542e-01,  1.4485e-02,  9.7482e-01, -2.3515e-01,  6.7111e-01,\n",
      "         -7.8194e-01, -8.9900e-01, -1.9186e-01, -5.4919e-02,  3.4735e-02,\n",
      "          1.8537e-01,  7.7000e-02,  9.8301e-01, -2.2325e-01,  5.3895e-01,\n",
      "          4.1103e-01,  9.8763e-01,  7.7442e-02, -1.7749e-01,  2.8833e-01,\n",
      "         -6.7789e-01,  3.5987e-01,  1.1326e-01, -2.4483e-01,  7.1545e-01,\n",
      "         -1.6362e-01,  1.1421e-01,  3.5360e-02, -2.9732e-02,  2.5463e-01,\n",
      "         -2.7908e-01,  9.9625e-01, -8.4144e-01, -9.3548e-02, -9.8464e-01,\n",
      "         -1.3688e-01,  9.4385e-01, -2.5925e-01, -8.0187e-02,  3.9113e-01,\n",
      "          4.1551e-02,  3.4670e-01,  4.4653e-03,  9.8116e-01,  1.1313e-01,\n",
      "          2.2766e-01,  1.9606e-01,  1.1483e-01,  9.9112e-01,  2.7459e-01,\n",
      "         -9.9269e-02,  8.3857e-02,  1.8127e-01,  4.7632e-02,  1.6454e-01,\n",
      "          5.8514e-03, -2.8422e-01, -9.9626e-02,  3.3749e-01, -8.8615e-01,\n",
      "         -1.9670e-01, -9.5962e-01,  9.5881e-01, -2.8467e-01,  1.1433e-01,\n",
      "         -9.7804e-01, -8.5199e-01, -9.8506e-01, -9.8728e-01,  5.5342e-02,\n",
      "          8.8158e-01, -9.8387e-01, -6.0640e-01,  1.4554e-01,  9.6649e-01,\n",
      "          1.6681e-02,  9.1532e-01,  9.9677e-01, -6.6448e-01,  1.8247e-01,\n",
      "          8.6146e-01,  3.8650e-03, -1.2586e-01,  5.9317e-02, -1.2873e-01,\n",
      "         -9.9102e-01, -8.0059e-02, -3.5522e-01,  4.6264e-02,  8.3596e-01,\n",
      "         -2.7899e-02,  4.2931e-02,  9.4356e-02, -9.6350e-03, -2.9923e-01,\n",
      "          1.2542e-01,  5.3024e-01, -1.6804e-01,  1.0591e-01,  2.3428e-01,\n",
      "          2.4766e-01,  7.9529e-03, -8.7780e-02, -1.4286e-01, -7.0539e-01,\n",
      "         -8.3393e-01,  8.1349e-01, -5.1150e-01, -1.9120e-01,  3.6922e-01,\n",
      "         -4.2675e-01,  1.3647e-01, -1.1472e-01,  9.9587e-01,  1.1480e-02,\n",
      "         -1.4997e-01,  1.6136e-01,  8.1899e-03,  4.9286e-02, -6.8057e-01,\n",
      "         -9.9958e-01, -5.5925e-02,  8.4099e-01,  9.3849e-01,  1.4833e-01,\n",
      "         -3.9352e-01,  1.3757e-02, -7.6756e-01,  2.8496e-01,  1.5424e-01,\n",
      "          8.8638e-01, -8.2030e-01,  7.0040e-01]])\n",
      "tensor([-2.2952e-01,  4.1768e-01,  3.3441e-02,  2.0379e-01, -6.2874e-02,\n",
      "         7.3399e-01, -6.7890e-01,  8.0131e-01,  4.0945e-01, -6.4637e-01,\n",
      "         2.0227e-01,  1.0747e-01,  7.0917e-01, -9.0679e-01,  6.1666e-01,\n",
      "         5.3996e-01,  2.5289e-01,  4.5406e-01, -1.5380e-01,  1.4911e-01,\n",
      "         7.6988e-01,  1.4341e-01,  7.5192e-01, -2.1346e-01, -1.5506e-01,\n",
      "         5.4964e-02,  1.2941e-01,  5.2119e-01,  8.0068e-01,  1.0001e+00,\n",
      "        -4.5376e-01,  1.5914e-01, -2.1611e-01,  2.1305e-01,  1.7096e-01,\n",
      "         2.0669e-01, -1.1605e+00, -8.8006e-01, -7.9505e-01,  8.0800e-01,\n",
      "        -2.3155e-01,  2.7760e-01,  8.9117e-01,  5.2419e-01, -1.0080e-01,\n",
      "        -6.8259e-01, -4.6383e-02, -6.0126e-01, -9.6604e-01,  3.6074e-02,\n",
      "         3.9236e-01,  3.7956e-01,  3.1761e-01, -1.3096e-01, -6.3218e-01,\n",
      "        -8.0918e-01, -9.9673e-01, -1.2385e-01, -2.5028e-01, -7.3378e-02,\n",
      "         6.8897e-01, -3.9152e-01,  9.2799e-01,  7.6603e-01,  2.3278e-01,\n",
      "        -6.2495e-02,  2.1636e-01,  1.8854e-01,  8.9051e-02, -8.6677e-02,\n",
      "         5.8783e-01,  3.2393e-01,  8.6143e-01, -5.0469e-01,  9.2768e-01,\n",
      "        -5.4523e-02, -2.8817e-01,  7.3177e-01, -2.2753e-01, -7.0129e-01,\n",
      "        -6.5049e-01, -3.1418e-01, -5.6599e-01,  4.5242e-01,  6.6247e-01,\n",
      "         8.8267e-02, -2.7780e-02, -4.9945e-01,  1.1161e+00,  5.2383e-02,\n",
      "         1.2913e+00,  2.7424e-01,  8.5213e-01,  4.1540e-01, -4.4217e-02,\n",
      "         9.2152e-02, -3.9520e-01,  5.4771e-02, -9.5701e-02, -1.8908e+01,\n",
      "        -6.0816e-02, -2.2985e-01,  7.1140e-01,  6.6753e-02, -1.7122e-01,\n",
      "         9.6041e-02, -3.6747e-01, -1.8762e-01, -7.2087e-02, -9.3328e-01,\n",
      "         3.2661e-02,  7.9908e-01, -2.2731e-01,  4.9731e-01,  4.2103e-01,\n",
      "        -5.4765e-01,  1.6745e-01,  5.9721e-01,  5.0106e-01,  3.4840e-03,\n",
      "        -1.7745e-01, -7.5898e-01, -3.6492e-01,  5.0673e-01,  2.8388e-01,\n",
      "         2.2839e-01,  9.1515e-01,  7.7580e-01, -1.6607e-01, -8.6427e-02,\n",
      "        -3.2529e-01,  2.3934e-01,  1.3577e-01, -2.3541e-01,  3.2278e-01,\n",
      "         1.9914e-01,  8.5006e-01, -2.9296e-01,  1.5167e-01,  4.6517e-01,\n",
      "        -5.3896e-01,  1.0420e-01, -3.2207e-01,  3.0579e-01,  3.6276e-01,\n",
      "         3.1147e-02, -2.8603e-01,  4.5077e-01, -4.7855e-02, -1.5327e+00,\n",
      "         6.8977e-01, -2.1658e-01,  8.4706e-02,  8.2512e-01, -3.0210e-01,\n",
      "         3.1802e-01,  1.7629e-01, -6.3647e-01,  5.3955e-02, -2.1005e-01,\n",
      "        -5.5701e-01,  4.5496e-02,  8.0887e-01,  8.1278e-01, -3.5245e-01,\n",
      "         9.6531e-02,  2.2566e-01, -1.2949e+00, -3.5233e-01, -7.2786e-01,\n",
      "         6.9024e-01, -4.2497e-01, -4.3492e-01,  3.9374e-01, -6.2711e-01,\n",
      "        -1.1644e-01,  7.0615e-01,  6.9894e-01, -1.3048e-01,  2.7691e-01,\n",
      "        -1.9902e-01,  4.9781e-01, -1.1073e+00,  2.9320e-01, -2.7215e-01,\n",
      "        -3.4711e-01, -1.2554e-01,  2.3552e-01,  4.7384e-01,  7.7626e-01,\n",
      "        -8.6659e-01,  6.8306e-01,  8.3776e-01, -5.3169e-01,  9.0282e-01,\n",
      "         1.2075e+00,  3.5944e-01,  1.4959e-01,  1.2623e+00, -7.8695e-01,\n",
      "        -3.3943e-01, -8.2518e-01,  2.8878e-01, -1.2874e-02,  1.1054e+00,\n",
      "        -4.7215e-01, -1.7523e-01, -4.7852e-01, -1.2042e-01,  1.1821e-01,\n",
      "         1.3554e-01, -3.9725e-01, -1.5966e-01,  8.2775e-01,  4.4856e-01,\n",
      "        -1.2429e-01, -1.4621e-01,  2.9381e-01,  1.8505e-01,  3.0973e-01,\n",
      "         2.7018e-01, -8.4412e-02,  3.2526e-02,  2.3422e-01,  4.8271e-01,\n",
      "         6.3973e-01,  4.1369e-01, -8.9371e-01,  5.5629e-01,  3.7052e-01,\n",
      "         4.5274e-01,  8.2455e-01,  3.5193e-02,  1.1333e+00, -8.6085e-03,\n",
      "        -5.5551e-01, -2.3907e-01, -6.8757e-01, -6.9027e-02,  2.0649e-01,\n",
      "         8.6636e-01,  1.0872e-01,  4.0397e-02,  5.3502e-01,  3.2957e-02,\n",
      "         4.8066e-01,  4.7538e-01, -9.5098e-01,  5.1937e-01,  1.9587e-01,\n",
      "         5.9071e-01,  9.8209e-01,  8.4806e-01, -1.0122e+00, -1.8424e-01,\n",
      "         2.4356e-01,  2.3485e+00, -7.4677e-01, -8.0792e-02,  2.1335e-01,\n",
      "         4.3520e-01,  3.9320e-01,  4.2033e-01,  5.1149e-01, -3.2835e-01,\n",
      "        -5.1186e-01, -2.2718e-01,  6.4330e-01, -1.9554e-01, -3.6242e-01,\n",
      "        -1.6152e+00, -9.5386e-01,  8.5447e-01,  2.2503e-01, -2.0289e-01,\n",
      "         6.3828e-01,  1.7172e-01,  4.2563e-01,  2.1230e-01,  2.4890e-01,\n",
      "         8.7666e-01,  4.6410e-01,  1.2540e-01,  1.5960e-03, -1.4173e+00,\n",
      "         4.8609e-01, -1.8536e-02, -1.4388e+00, -2.6672e-01,  3.7863e-01,\n",
      "        -4.9682e-01,  2.9136e-01, -1.0546e-01, -5.0527e-01, -2.1072e-01,\n",
      "         1.6699e-02,  1.3273e-01, -3.5189e-01, -4.2389e-01, -2.9432e-01,\n",
      "         2.9604e-01,  2.2504e-01,  5.3544e-01,  4.4816e-01, -1.7756e-01,\n",
      "        -1.3981e-01,  3.5620e-01,  3.4850e-01, -2.5792e-03, -4.8089e-01,\n",
      "        -1.7798e-01,  4.5466e-01, -5.0482e-01, -8.2326e-01,  8.9124e-01,\n",
      "        -5.0088e-01, -1.7046e-01,  1.0271e+00, -9.5893e-02, -6.1411e-01,\n",
      "        -2.5026e-01, -4.2154e-01, -4.9590e-01,  3.5588e-01,  3.9997e-01,\n",
      "         6.0306e-01, -3.9085e-02, -1.7551e-01,  1.2030e-01,  9.0178e-02,\n",
      "         2.3806e-01,  1.2029e-01,  6.6499e+00, -4.2980e-01,  2.6745e-01,\n",
      "         3.2723e-01,  1.3847e+00,  1.3601e-01,  1.0263e-02,  4.3117e-01,\n",
      "        -2.7187e-01,  8.5238e-02, -7.0925e-01,  1.3417e-02, -8.0248e-01,\n",
      "         4.0358e-01, -1.3747e+00,  3.6396e-01,  3.2249e-01, -4.9653e-01,\n",
      "        -1.0146e-01,  4.2580e-02,  3.1052e-01, -1.5799e-01, -5.5733e-01,\n",
      "        -2.6450e-01, -5.5826e-02,  2.4028e-02,  5.8378e-01, -1.9024e-01,\n",
      "        -5.0817e-01,  9.1667e-01, -4.4036e-01,  1.9158e-01, -6.1874e-02,\n",
      "        -3.2338e-01,  6.8611e-01, -7.2978e-01, -5.8805e-01,  3.4227e-01,\n",
      "        -2.6391e-01,  7.7843e-02, -1.0933e-01,  1.9162e-01, -3.3752e-01,\n",
      "         4.6223e-01,  7.6260e-02,  1.7988e-01,  1.5103e-01, -6.9164e-02,\n",
      "         5.6641e-01, -6.2591e-01, -4.9411e-01,  3.7823e-01,  3.5957e-01,\n",
      "        -1.0814e+00,  8.7527e-01, -7.0515e-01,  2.9299e-01, -1.6909e-02,\n",
      "         5.6707e-01,  1.9173e-02, -7.8672e-01,  5.0221e-01,  4.3373e-01,\n",
      "        -3.4187e-01, -2.1002e-01,  4.2137e-01,  5.1159e-01,  3.1433e-01,\n",
      "        -1.9584e-01, -4.6775e-01,  9.5662e-01, -5.0628e-01, -6.6887e-01,\n",
      "         8.0096e-04,  1.1570e-01,  2.6426e-01, -4.0213e-01, -4.3618e-01,\n",
      "         3.4574e-01, -3.6108e-01,  1.2279e-01,  9.7157e-01,  6.6288e-01,\n",
      "         7.0585e-01, -4.5992e-01,  5.4740e-01,  7.2576e-01, -1.3191e-01,\n",
      "        -4.8546e-01, -1.1527e+00, -1.4155e-01,  3.7789e-01, -3.8131e-01,\n",
      "        -3.6996e-01, -1.2467e-01, -4.3411e-01, -3.2357e-02,  1.6756e-01,\n",
      "         2.3867e-01, -7.6957e-01, -2.8180e-01,  1.2272e-01, -3.3722e-01,\n",
      "         1.3341e+00,  1.3674e+00, -4.3279e-01,  1.0076e+00, -4.9676e-01,\n",
      "         3.6053e-01, -3.6560e-01,  2.6190e-01,  7.1382e-01,  1.2618e-01,\n",
      "        -4.4331e-01, -4.2301e-01,  1.2555e-01,  7.6883e-02,  1.1127e+00,\n",
      "        -1.6281e-02, -1.3922e-01,  4.4631e-01, -1.1097e-02, -1.1638e-01,\n",
      "         1.7091e-01, -6.3596e-01,  3.5593e-01, -3.6035e-01,  5.4873e-02,\n",
      "         4.0986e-01, -7.1237e-01,  7.4446e-01,  4.8255e-01, -7.2246e-01,\n",
      "        -4.5633e-01,  6.9379e-01,  2.5163e-02,  4.2506e-01,  2.5158e-01,\n",
      "         1.3887e-01,  8.7101e-01, -1.4536e-01, -9.1480e-01, -1.7178e-01,\n",
      "         1.0400e-01,  6.0240e-02, -7.5347e-01, -6.4263e-01, -3.5488e-01,\n",
      "        -1.1399e+00, -3.4718e-01,  9.5355e-01, -1.3770e-01, -1.2547e+00,\n",
      "        -1.5209e-01,  8.9913e-02,  3.3752e-01, -1.5833e-01, -7.3608e-03,\n",
      "         9.2255e-02, -3.2785e-01,  1.6048e-02, -2.4471e-01, -2.6310e-03,\n",
      "         1.9902e-01,  4.6842e-01, -4.5081e-01, -6.9541e-01,  2.1450e-01,\n",
      "         1.5967e-01, -9.9816e-01,  1.9053e-01, -2.0490e-01, -1.9766e-02,\n",
      "        -1.5155e+00,  8.3755e-01, -1.8152e-01,  2.0618e-01,  5.3083e+00,\n",
      "         1.1647e+00, -8.7228e-01,  9.8803e-02, -7.0552e-01,  1.3771e-01,\n",
      "        -6.6194e-01, -1.8528e-01, -4.6045e-01,  1.0245e+00,  1.4610e-01,\n",
      "         8.2951e-01,  3.0664e-01,  3.7069e-01, -5.3353e-02, -4.3506e-01,\n",
      "         6.2671e-01, -6.2920e-01, -3.5359e-01, -4.1738e-01, -2.9535e-01,\n",
      "         8.4187e-01, -7.9394e-02, -8.6402e-02,  1.2179e-01, -5.9287e-01,\n",
      "        -3.1913e-01, -3.3819e-01, -4.4643e-01,  7.5255e-01, -5.1742e-01,\n",
      "         8.9478e-01,  3.0943e-01, -6.7739e-01, -5.3134e-02,  4.3002e-01,\n",
      "        -7.6686e-01,  1.1562e-01,  8.5802e-01, -6.8355e-02,  4.4779e-01,\n",
      "        -3.3518e-02, -1.1427e-02, -6.5345e-01, -2.0707e-01, -1.4568e-01,\n",
      "        -6.2533e-02, -1.1054e+00,  9.3774e-02, -5.5588e-01,  4.7327e-01,\n",
      "        -6.0391e-02,  7.3272e-01, -2.4185e+00,  1.6305e+00,  4.2136e-02,\n",
      "         1.1670e+00,  9.0885e-01, -8.7962e-01, -3.8805e-01, -1.1540e+00,\n",
      "        -3.4151e-01,  6.2773e-02, -6.9073e-01,  3.5504e-01,  2.6008e-01,\n",
      "        -2.1901e-01,  9.8666e-01, -3.2567e-01, -1.7968e-01, -1.7628e+00,\n",
      "        -7.4008e-01, -1.0815e-01,  5.9798e-01, -7.4416e-01, -3.3820e-01,\n",
      "         1.2343e+00, -3.3657e-01,  2.4190e-02, -2.9847e-01, -2.2179e-01,\n",
      "         1.8052e-01, -2.6689e-01,  3.5588e-01, -3.9507e-01,  2.3376e-02,\n",
      "         9.7679e-02,  2.3414e-01,  8.9074e-01, -3.1419e-01, -5.8380e-01,\n",
      "        -9.5800e-01, -5.0045e-01, -2.6511e-01,  8.8684e-01, -9.0235e-01,\n",
      "        -2.7167e-01, -3.5760e-01, -9.4277e-01, -5.3791e-01, -5.2385e-01,\n",
      "         4.8842e-01, -4.6270e-01,  5.7423e-01,  3.4936e-02,  1.3942e-01,\n",
      "        -3.7627e-01,  3.0238e-01, -1.2831e-01,  1.9263e-01,  3.7332e-01,\n",
      "         3.6583e-01,  1.3258e-01,  1.6113e-01, -2.1812e-01, -6.6892e-02,\n",
      "        -8.6387e-02, -1.0652e+00,  6.6749e-01, -2.7895e-01,  2.7203e-01,\n",
      "         2.0847e-01,  5.9294e-01, -1.0152e+00, -8.6351e-01, -1.1761e+00,\n",
      "        -2.3136e-01, -1.2956e+00, -2.1624e-01, -6.7083e-01, -2.2489e-01,\n",
      "         2.9891e-01,  3.2556e-01,  4.7074e-02,  1.6642e+00,  4.1921e-01,\n",
      "         1.0163e-01, -1.3437e-01,  5.0922e-01, -2.4678e-01,  2.8013e-01,\n",
      "        -3.7390e-01,  1.5235e-02,  7.6492e-01,  1.6140e+00, -3.9694e-01,\n",
      "        -4.6895e-01, -8.3862e-01,  3.1540e-01,  3.6398e-01, -1.2036e+00,\n",
      "         1.4091e+00,  2.9363e-01,  1.6928e-02,  3.7250e-01,  3.4068e-01,\n",
      "         4.9972e-01,  9.7679e-02, -7.6625e-02, -4.8857e-01,  3.5980e-01,\n",
      "        -2.5493e-01, -2.1495e-01,  3.3502e-01, -1.1549e-01, -6.4439e-02,\n",
      "         4.1965e-03, -4.4133e-01, -7.4713e-01, -5.3244e-01, -4.3970e-01,\n",
      "         8.4904e-01, -4.1826e-01,  9.0711e-01, -1.5650e-01, -5.8749e-01,\n",
      "        -7.8398e-02,  4.4707e-03, -4.1994e-01, -9.7566e-01, -2.9661e-01,\n",
      "        -5.1494e-01,  3.7756e-02,  1.2190e-01, -4.2831e-01, -1.3897e-01,\n",
      "         1.1013e+00, -1.8306e-01,  2.4251e-01, -6.2381e-01, -7.5042e-01,\n",
      "         3.2599e-01,  3.6974e-01,  6.0716e-01,  4.1650e-01,  1.9437e-01,\n",
      "        -8.5765e-01,  1.5754e-01, -3.1056e-01,  7.0012e-01, -1.5283e-01,\n",
      "         2.2714e-02, -9.9481e-01, -2.2447e-01, -3.3984e-01,  5.2403e-01,\n",
      "        -1.4280e-01, -8.2202e-01,  6.0736e-04, -6.4015e-01,  9.1189e-01,\n",
      "         4.5678e-01, -1.8931e-01, -7.9319e-01,  3.5132e-01, -4.5889e-01,\n",
      "        -2.4156e-01,  6.7878e-01, -7.7126e-01, -4.3305e-01, -2.4869e-02,\n",
      "        -2.7917e-01,  5.8166e-01, -8.7276e-01, -4.7715e-02,  2.5240e-01,\n",
      "        -5.2294e-01,  1.7034e-01,  9.6443e-01, -3.7027e-01, -2.4972e-01,\n",
      "         7.5313e-01, -5.1354e-01, -3.4297e-01, -5.9010e-02,  8.6692e-02,\n",
      "        -5.3512e-01,  4.9765e-01, -3.8822e-01, -5.3957e-01,  8.4200e-01,\n",
      "         9.6736e-01,  6.7739e-01, -2.6159e-01,  4.0736e-01, -3.6388e-01,\n",
      "         7.3756e-01,  4.7435e-01,  3.8387e-01,  9.6486e-02,  7.3080e-01,\n",
      "        -2.4748e-01, -6.9965e-01, -7.8191e-01,  1.0842e-01, -4.5500e-01,\n",
      "         1.1136e+00,  2.9973e-01,  2.5628e-01])\n"
     ]
    }
   ],
   "source": [
    "# Predict hidden states features for each layer\n",
    "with torch.no_grad():\n",
    "    encoded_layers, hidden = model(tokens_tensor, output_all_encoded_layers=False)\n",
    "assert len(encoded_layers) == 1\n",
    "print(encoded_layers[0].size())\n",
    "print(hidden.size())\n",
    "print(encoded_layers[0][-1,:].size())\n",
    "print(hidden)\n",
    "print(encoded_layers[0][-1,:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we try to get representation of multiple sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['[CLS]', '私', 'は', '２０', '歳', 'で', '学生', 'です', '。'], ['[CLS]', '数学', 'の', '最も', '普通の', '定義', 'と', 'して', 'は', '、', '「', '数', 'および', '図形', 'に', 'ついて', 'の', '学問', '」', 'と', 'いう', 'もの', 'が', 'ある', '。']]\n",
      "[[    2  1038     9   183   205    13  1098  3338     7     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0]\n",
      " [    2  2938     5   476  7078  1315    12    19     9     6    24   145\n",
      "    186 17201     8   130     5  5476    25    12    56    60    11    38\n",
      "      7]]\n",
      "torch.Size([2, 25])\n",
      "tensor([[    2,  1038,     9,   183,   205,    13,  1098,  3338,     7,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0],\n",
      "        [    2,  2938,     5,   476,  7078,  1315,    12,    19,     9,     6,\n",
      "            24,   145,   186, 17201,     8,   130,     5,  5476,    25,    12,\n",
      "            56,    60,    11,    38,     7]])\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "texts = [\"私は２０歳で学生です。\", \"数学の最も普通の定義としては、「数および図形についての学問」というものがある。\"]\n",
    "texts = list(map(tokenize, texts))\n",
    "tokenized_texts = list(map(lambda t: ['[CLS]'] + tokenizer.tokenize(t), texts))\n",
    "\n",
    "print(tokenized_texts)\n",
    "\n",
    "indexed_tokens = list(map(tokenizer.convert_tokens_to_ids, tokenized_texts))\n",
    "maxlen = max([len(s) for s in indexed_tokens])\n",
    "\n",
    "padded_tokens = pad_sequences(indexed_tokens, maxlen=maxlen, truncating=\"post\", padding=\"post\", dtype=\"int\")\n",
    "print(padded_tokens)\n",
    "\n",
    "tokens_tensor = torch.tensor(padded_tokens)\n",
    "tokens_tensor = tokens_tensor.to(device)\n",
    "\n",
    "print(tokens_tensor.size())\n",
    "print(tokens_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 25, 768])\n",
      "torch.Size([2, 768])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    encoded_layers, hidden = model(tokens_tensor, output_all_encoded_layers=True)\n",
    "print(encoded_layers[-2].size())\n",
    "print(hidden.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see some outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.8578, -0.3807, -0.3330,  ..., -0.4755,  0.5379, -0.8880],\n",
      "        [ 0.0526, -0.4467, -0.4227,  ...,  0.5322, -0.0773,  0.0536]])\n",
      "tensor([[ 0.8578, -0.3807, -0.3330,  ..., -0.4755,  0.5379, -0.8880],\n",
      "        [ 0.0526, -0.4467, -0.4227,  ...,  0.5322, -0.0773,  0.0536]])\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "def avg(src_h):\n",
    "    batch_size = src_h.size(0)\n",
    "    src_length = src_h.size(1)\n",
    "    avg_weights = Variable(torch.ones(batch_size, 1, src_length)).cuda() if torch.cuda.is_available() and self.useGpu else Variable(torch.ones(batch_size, 1, src_length)) / src_length\n",
    "    src_h_t = torch.bmm(avg_weights, src_h)\n",
    "    src_h_t = src_h_t[:,0,:]\n",
    "    return src_h_t\n",
    "\n",
    "print( torch.mean(encoded_layers[-2], 1) )\n",
    "print( avg(encoded_layers[-2]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we use ```BertForMaskedLM``` to predict tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_pretrained_bert.modeling:loading archive file /Users/minhpqn/workspace/Japanese_L-12_H-768_A-12_E-30_BPE\n",
      "INFO:pytorch_pretrained_bert.modeling:Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 32006\n",
      "}\n",
      "\n",
      "INFO:pytorch_pretrained_bert.modeling:Weights from pretrained model not used in BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(32006, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=32006, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load pre-trained model (weights)\n",
    "model = BertForMaskedLM.from_pretrained(path_to_pretrained_model, cache_dir=None)\n",
    "model.eval()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "は\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    predictions = model(tokens_tensor)\n",
    "\n",
    "predicted_index = torch.argmax(predictions[0, masked_index]).item()\n",
    "predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]\n",
    "print(predicted_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using BERT with SentencePiece for Japanese text\n",
    "\n",
    "We now use BERT with SentencePiece for Japanese. The point is that you will use the pre-trained BERT model in which they used sentencepiece for subword tokenization. The pretrained BERT model with sentencepiece is available on [https://github.com/yoheikikuta/bert-japanese](https://github.com/yoheikikuta/bert-japanese)\n",
    "\n",
    "You need to to install [sentencepiece](https://github.com/google/sentencepiece) in order to play with that pre-trained model.\n",
    "\n",
    "Pre-trained BERT model in [https://github.com/yoheikikuta/bert-japanese](https://github.com/yoheikikuta/bert-japanese) can only be used with Tensorflow implementation of BERT. In order to use with Pytorch implementation, use need to convert the Tensorflow model in to the format which is compatible with Pytorch. I followed the instruction in [https://github.com/huggingface/pytorch-pretrained-BERT#Command-line-interface](https://github.com/huggingface/pytorch-pretrained-BERT#Command-line-interface) to convert the model into Pytorch saved model file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_pretrained_bert.modeling:loading archive file /Users/minhpqn/nlp/data/japanese/bert/bert-wiki-ja\n",
      "INFO:pytorch_pretrained_bert.modeling:Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(32000, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): BertLayerNorm()\n",
       "    (dropout): Dropout(p=0.1)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from pytorch_pretrained_bert import BasicTokenizer, BertTokenizer, BertModel, BertForMaskedLM\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "        \n",
    "path_to_pretrained_model = '/Users/minhpqn/nlp/data/japanese/bert/bert-wiki-ja'\n",
    "# path_to_pretrained_model = '/Users/minhpqn/nlp/data/japanese/bert/multi_cased_L-12_H-768_A-12'\n",
    "\n",
    "model = BertModel.from_pretrained(path_to_pretrained_model, cache_dir=None)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a tokenizer using sentencepiece. We use the module `tokenization_sentencepiece.py` from [https://github.com/yoheikikuta/bert-japanese](https://github.com/yoheikikuta/bert-japanese)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded a trained SentencePiece model.\n",
      "['[CLS]', '▁', '数学', 'の', '最も', '普通', 'の定義', 'としては', '、「', '数', 'および', '図形', 'についての', '学問', '」', 'という', 'ものがある', '。']\n",
      "['[CLS]', '▁', '数学', 'の', '最も', '普通', 'の定義', 'としては', '[MASK]', '数', 'および', '図形', 'についての', '学問', '」', 'という', 'ものがある', '。']\n",
      "[4, 9, 4560, 10, 1016, 2334, 9070, 512, 6, 181, 144, 24031, 3226, 6078, 21, 49, 5805, 8]\n",
      "torch.Size([1, 18])\n",
      "torch.Size([1, 18, 768])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tokenization_sentencepiece as tokenization\n",
    "\n",
    "model_file = os.path.join(path_to_pretrained_model, 'wiki-ja.model')\n",
    "vocab_file = os.path.join(path_to_pretrained_model, 'wiki-ja.vocab')\n",
    "\n",
    "tokenizer = tokenization.FullTokenizer(model_file=model_file, vocab_file=vocab_file, do_lower_case=False)\n",
    "text = \"数学の最も普通の定義としては、「数および図形についての学問」というものがある。\"\n",
    "tokenized_text = tokenizer.tokenize(text)\n",
    "tokenized_text.insert(0,'[CLS]')\n",
    "print(tokenized_text)\n",
    "\n",
    "masked_index = 8\n",
    "tokenized_text[masked_index] = '[MASK]'\n",
    "print(tokenized_text)\n",
    "\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "tokens_tensor = tokens_tensor.to(device)\n",
    "\n",
    "print(indexed_tokens)\n",
    "print(tokens_tensor.size())\n",
    "\n",
    "# Let's see how to use BertModel to get hidden states.\n",
    "# Predict hidden states features for each layer\n",
    "with torch.no_grad():\n",
    "    encoded_layers, hidden = model(tokens_tensor, output_all_encoded_layers=True)\n",
    "assert len(encoded_layers) == 12\n",
    "print(encoded_layers[0].size())\n",
    "print(hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We try to just output the last encoded layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([18, 768])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    encoded_layers, _ = model(tokens_tensor, output_all_encoded_layers=False)\n",
    "print(encoded_layers[0].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we use BertForMaskedLM to predict tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_pretrained_bert.modeling:loading archive file /Users/minhpqn/nlp/data/japanese/bert/bert-wiki-ja\n",
      "INFO:pytorch_pretrained_bert.modeling:Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "INFO:pytorch_pretrained_bert.modeling:Weights from pretrained model not used in BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(32000, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=32000, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load pre-trained model (weights)\n",
    "model = BertForMaskedLM.from_pretrained(path_to_pretrained_model, cache_dir=None)\n",
    "model.eval()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "、「\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    predictions = model(tokens_tensor)\n",
    "\n",
    "predicted_index = torch.argmax(predictions[0, masked_index]).item()\n",
    "predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]\n",
    "print(predicted_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- BERT with SentencePiece for Japanese text. [https://github.com/yoheikikuta/bert-japanese](https://github.com/yoheikikuta/bert-japanese)\n",
    "- [pytorch-pretrained-BERT](https://github.com/huggingface/pytorch-pretrained-BERT)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
